{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook : Évaluation du modèle sur différentes périodes avec Prometheus (Mensuelle)\n",
    "Ce notebook charge un modèle de machine learning sauvegardé, évalue les performances \n",
    "du modèle mois par mois et enregistre les métriques dans Prometheus. A la fin, travaille avec le modèle au jour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Importation des bibliothèques et configuration Prometheus\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from prometheus_client import Gauge, start_http_server\n",
    "import mlflow\n",
    "# serveur HTTP Prometheus sur le port 8000\n",
    "start_http_server(8000)\n",
    "\n",
    "# Configuration des métriques Prometheus\n",
    "accuracy_gauge = Gauge('model_accuracy', 'Model Accuracy', ['month'])\n",
    "f1_gauge = Gauge('model_f1_score', 'Model F1 Score', ['month'])\n",
    "precision_gauge = Gauge('model_precision', 'Model Precision', ['month'])\n",
    "recall_gauge = Gauge('model_recall', 'Model Recall', ['month'])\n",
    "loss_gauge = Gauge('model_loss', 'Model Loss', ['month'])\n",
    "\n",
    "# Statistiques descriptives pour les colonnes numériques\n",
    "mean_gauge = Gauge('numeric_column_mean', 'Mean of numeric column', ['column', 'month'])\n",
    "std_gauge = Gauge('numeric_column_std', 'Std of numeric column', ['column', 'month'])\n",
    "min_gauge = Gauge('numeric_column_min', 'Min of numeric column', ['column', 'month'])\n",
    "max_gauge = Gauge('numeric_column_max', 'Max of numeric column', ['column', 'month'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artefacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Artefacts stockés (Modèles et données)\n",
    "def load_mlflow_artifact(artifact_path):\n",
    "    local_path = mlflow.artifacts.download_artifacts(artifact_path)\n",
    "    with open(local_path, 'rb') as f:\n",
    "        return joblib.load(f)\n",
    "\n",
    "# Modèles et les artefacts\n",
    "X_reduced = load_mlflow_artifact(\"mlflow_artifacts/X_reduced.pkl\")\n",
    "y = load_mlflow_artifact(\"mlflow_artifacts/y.pkl\")\n",
    "model_path = \"mlflow_artifacts/bow_svd_model.h5\"\n",
    "\n",
    "# Modèle depuis MLflow\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Compile le modèle\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation des données par mois\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divise les données en périodes mensuelles\n",
    "\n",
    "df = pd.read_csv('db/cleaned_data_sample.csv')\n",
    "df['CreationDate'] = pd.to_datetime(df['CreationDate'])\n",
    "df['month'] = df['CreationDate'].dt.to_period('M')  # Ajout d'une colonne \"mois\"\n",
    "\n",
    "# Obtient la liste des mois uniques dans les données\n",
    "unique_months = df['month'].unique()\n",
    "\n",
    "# Obtient les données d'un mois spécifique\n",
    "def get_data_for_month(df, X_reduced, y, month):\n",
    "    df_month = df[df['month'] == month]\n",
    "    X_month = X_reduced[:len(df_month)]\n",
    "    y_month = y[:len(df_month)]\n",
    "    return X_month, y_month, df_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistiques descriptives et enregistrement des métriques dans Prometheus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def log_descriptive_statistics(df, month):\n",
    "    \"\"\"Calcule et enregistre les statistiques descriptives des colonnes numériques dans Prometheus.\"\"\"\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    descriptive_stats = df[numeric_columns].describe()\n",
    "\n",
    "    for col in numeric_columns:\n",
    "        mean_gauge.labels(column=col, month=str(month)).set(descriptive_stats.loc['mean', col])\n",
    "        std_gauge.labels(column=col, month=str(month)).set(descriptive_stats.loc['std', col])\n",
    "        min_gauge.labels(column=col, month=str(month)).set(descriptive_stats.loc['min', col])\n",
    "        max_gauge.labels(column=col, month=str(month)).set(descriptive_stats.loc['max', col])\n",
    "\n",
    "    print(f\"Statistiques descriptives pour {month} enregistrées dans Prometheus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Évaluation du modèle sans réentraînement (Data Drift et Model Drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, month):\n",
    "    \"\"\"Évalue les performances du modèle et enregistre les métriques dans Prometheus.\"\"\"\n",
    "    # Prédictions du modèle\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = (y_pred > 0.5).astype(\"int32\")\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # Enregistrement des métriques dans Prometheus\n",
    "    accuracy_gauge.labels(month=str(month)).set(accuracy)\n",
    "    f1_gauge.labels(month=str(month)).set(f1)\n",
    "    precision_gauge.labels(month=str(month)).set(precision)\n",
    "    recall_gauge.labels(month=str(month)).set(recall)\n",
    "    loss_gauge.labels(month=str(month)).set(loss)\n",
    "\n",
    "    print(f\"{month} - Loss: {loss}, Accuracy: {accuracy}, F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation et statistiques mensuelles\n",
    "Boucle sur chaque mois, calcule les statistiques descriptives et évalue le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiques descriptives pour 2023-04 enregistrées dans Prometheus.\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "2023-04 - Loss: 0.11331631243228912, Accuracy: 0.612130880355835, F1 Score: 0.4832492579951344\n",
      "Statistiques descriptives pour 2023-10 enregistrées dans Prometheus.\n",
      "\u001b[1m 1/34\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "2023-10 - Loss: 0.11111187189817429, Accuracy: 0.612994372844696, F1 Score: 0.4857132792692833\n",
      "Statistiques descriptives pour 2024-04 enregistrées dans Prometheus.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step\n",
      "2024-04 - Loss: 0.11695609241724014, Accuracy: 0.5615763664245605, F1 Score: 0.43992586971288794\n",
      "Statistiques descriptives pour 2023-02 enregistrées dans Prometheus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step\n",
      "2023-02 - Loss: 0.11363290995359421, Accuracy: 0.6114063262939453, F1 Score: 0.4813414372796587\n",
      "Statistiques descriptives pour 2023-07 enregistrées dans Prometheus.\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step\n",
      "2023-07 - Loss: 0.11282694339752197, Accuracy: 0.6106557250022888, F1 Score: 0.48093500165362696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiques descriptives pour 2023-03 enregistrées dans Prometheus.\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step\n",
      "2023-03 - Loss: 0.11386105418205261, Accuracy: 0.6129925847053528, F1 Score: 0.47939201261575015\n",
      "Statistiques descriptives pour 2023-09 enregistrées dans Prometheus.\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "2023-09 - Loss: 0.11184454709291458, Accuracy: 0.6122449040412903, F1 Score: 0.48215279051658444\n",
      "Statistiques descriptives pour 2023-08 enregistrées dans Prometheus.\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step\n",
      "2023-08 - Loss: 0.11210975050926208, Accuracy: 0.6092097163200378, F1 Score: 0.48009730265439143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiques descriptives pour 2023-06 enregistrées dans Prometheus.\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step\n",
      "2023-06 - Loss: 0.112894706428051, Accuracy: 0.6108193397521973, F1 Score: 0.48128901216956\n",
      "Statistiques descriptives pour 2023-05 enregistrées dans Prometheus.\n",
      "\u001b[1m 1/72\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 16ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "2023-05 - Loss: 0.11251611262559891, Accuracy: 0.614235520362854, F1 Score: 0.4833825032258061\n",
      "Statistiques descriptives pour 2023-11 enregistrées dans Prometheus.\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step\n",
      "2023-11 - Loss: 0.11183109879493713, Accuracy: 0.6107226014137268, F1 Score: 0.47414520622381273\n",
      "Statistiques descriptives pour 2023-01 enregistrées dans Prometheus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step\n",
      "2023-01 - Loss: 0.1135137602686882, Accuracy: 0.6132665872573853, F1 Score: 0.48091770903037734\n",
      "Statistiques descriptives pour 2024-01 enregistrées dans Prometheus.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step\n",
      "2024-01 - Loss: 0.11714627593755722, Accuracy: 0.5795847773551941, F1 Score: 0.4471346946791795\n",
      "Statistiques descriptives pour 2023-12 enregistrées dans Prometheus.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step\n",
      "2023-12 - Loss: 0.11623089760541916, Accuracy: 0.5947204828262329, F1 Score: 0.44987346602853673\n",
      "Statistiques descriptives pour 2024-02 enregistrées dans Prometheus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step\n",
      "2024-02 - Loss: 0.11774858087301254, Accuracy: 0.5919661521911621, F1 Score: 0.43775472788250624\n",
      "Statistiques descriptives pour 2024-07 enregistrées dans Prometheus.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "2024-07 - Loss: 0.10881894826889038, Accuracy: 0.6393442749977112, F1 Score: 0.31157329598506067\n",
      "Statistiques descriptives pour 2024-03 enregistrées dans Prometheus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "2024-03 - Loss: 0.12326657027006149, Accuracy: 0.5766870975494385, F1 Score: 0.4179382479811379\n",
      "Statistiques descriptives pour 2024-06 enregistrées dans Prometheus.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step\n",
      "2024-06 - Loss: 0.12003084272146225, Accuracy: 0.5662650465965271, F1 Score: 0.35768440709617183\n",
      "Statistiques descriptives pour 2024-05 enregistrées dans Prometheus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step\n",
      "2024-05 - Loss: 0.1151018887758255, Accuracy: 0.5849056839942932, F1 Score: 0.4407756132756133\n",
      "Statistiques descriptives pour 2024-09 enregistrées dans Prometheus.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "2024-09 - Loss: 0.11718586832284927, Accuracy: 0.6190476417541504, F1 Score: 0.18846153846153843\n",
      "Statistiques descriptives pour 2024-08 enregistrées dans Prometheus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "2024-08 - Loss: 0.11250752955675125, Accuracy: 0.6578947305679321, F1 Score: 0.29666666666666675\n",
      "Statistiques descriptives pour 2024-10 enregistrées dans Prometheus.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "2024-10 - Loss: 0.22167757153511047, Accuracy: 0.0, F1 Score: 0.0\n",
      "Évaluation mensuelle du modèle enregistrée dans Prometheus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "for month in unique_months:\n",
    "    # Données pour le mois actuel\n",
    "    X_month, y_month, df_month = get_data_for_month(df, X_reduced, y, month)\n",
    "\n",
    "    # Statistiques descriptives pour ce mois\n",
    "    log_descriptive_statistics(df_month, month)\n",
    "\n",
    "    # Evaluation du modèle pour ce mois\n",
    "    evaluate_model(model, X_month, y_month, month)\n",
    "\n",
    "# Résultats\n",
    "print(\"Évaluation mensuelle du modèle enregistrée dans Prometheus.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "gauge metric is missing label values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Utiliser les données actuelles pour l'évaluation\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mevaluate_model_and_log_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m, in \u001b[0;36mevaluate_model_and_log_metrics\u001b[1;34m(model, X_test, y_test)\u001b[0m\n\u001b[0;32m     14\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Enregistrement des métriques dans Prometheus\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43maccuracy_gauge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccuracy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m f1_gauge\u001b[38;5;241m.\u001b[39mset(f1)\n\u001b[0;32m     19\u001b[0m precision_gauge\u001b[38;5;241m.\u001b[39mset(precision)\n",
      "File \u001b[1;32mc:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\prometheus_client\\metrics.py:434\u001b[0m, in \u001b[0;36mGauge.set\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set gauge to the given value.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 434\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_not_observable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_most_recent:\n\u001b[0;32m    436\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28mfloat\u001b[39m(value), timestamp\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime())\n",
      "File \u001b[1;32mc:\\Users\\leenc\\Documents\\openclassrooms\\p5\\.venv\\Lib\\site-packages\\prometheus_client\\metrics.py:101\u001b[0m, in \u001b[0;36mMetricWrapperBase._raise_if_not_observable\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_if_not_observable\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# Functions that mutate the state of the metric, for example incrementing\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# a counter, will fail if the metric is not observable, because only if a\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# metric is observable will the value be initialized.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_observable():\n\u001b[1;32m--> 101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m metric is missing label values\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_type))\n",
      "\u001b[1;31mValueError\u001b[0m: gauge metric is missing label values"
     ]
    }
   ],
   "source": [
    "def evaluate_model_and_log_metrics(model, X_test, y_test):\n",
    "    \"\"\"Évalue les performances du modèle et enregistre les métriques dans Prometheus.\"\"\"\n",
    "    # Prédiction et évaluation du modèle\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = (y_pred > 0.5).astype(\"int32\")\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Enregistrement des métriques dans Prometheus avec le label \"global\" pour indiquer que c'est une évaluation générale\n",
    "    accuracy_gauge.labels(month=\"global\").set(accuracy)\n",
    "    f1_gauge.labels(month=\"global\").set(f1)\n",
    "    precision_gauge.labels(month=\"global\").set(precision)\n",
    "    recall_gauge.labels(month=\"global\").set(recall)\n",
    "    loss_gauge.labels(month=\"global\").set(loss)\n",
    "\n",
    "    print(f\"Loss: {loss}, Accuracy: {accuracy}, F1 Score: {f1}, Precision: {precision}, Recall: {recall}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
